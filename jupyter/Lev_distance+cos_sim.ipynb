{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMz4c4qcbcgO"
      },
      "source": [
        "The objective of this colab is to take the levenshtein distance of all the hittite words and the word2vec cosine similiarity of the english words and then perform a check to see to what extent the levenshtein distance of two particular words is similar in magnitude to the cosine similarity.\n",
        "\n",
        "\n",
        "1. Read in Json as two lists\n",
        "2. run LD on one list and save in a matrix\n",
        "3. run countVectorization on english words and save cos similarity in matrix\n",
        "4. run check to see if correlation between cos similarity of two english word and LD of two hittite words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ngzsgVbdOZ_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LR2BhDBzdeGM"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "dataset_file_path = \"/content/drive/MyDrive/Colab Notebooks/NLLB_files/filename-2.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1B2JgerhHG2"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yB9E_n5VbX3Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "# dataset = []\n",
        "# with open(dataset_file_path) as F:\n",
        "#   for l in F.readlines():\n",
        "#     l = json.loads(l)\n",
        "#     newL = dict()\n",
        "#     if l['Hittite']:\n",
        "#       newL['src_lang'] = l['Hittite'] # Hittite is source language\n",
        "#       newL['tgt_lang'] = l['Translation'] # English is target language\n",
        "#     dataset.append(newL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0oYZ5IhFGmY"
      },
      "outputs": [],
      "source": [
        "dataset = []\n",
        "with open(dataset_file_path, 'r') as file:\n",
        "    csv_reader = csv.DictReader(file)\n",
        "    for row in csv_reader:\n",
        "        new_row = {}\n",
        "        if row.get('src_lang') and row.get('tgt_lang'):  # Check if both columns have values\n",
        "            new_row['src_lang'] = row['src_lang']  # 'src_lang' corresponds to 'Hittite'\n",
        "            new_row['tgt_lang'] = row['tgt_lang']  # 'tgt_lang' corresponds to 'Translation'\n",
        "            dataset.append(new_row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ncf4SVj4du1h"
      },
      "source": [
        "run levenshtein distance between all hittite words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DOOOON-duck",
        "outputId": "d583055f-bf2b-485c-ad18-bd179b3bd999"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting levenshtein\n",
            "  Downloading Levenshtein-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=3.1.0 (from levenshtein)\n",
            "  Downloading rapidfuzz-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, levenshtein\n",
            "Successfully installed levenshtein-0.23.0 rapidfuzz-3.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install levenshtein\n",
        "from Levenshtein import distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHr0YQgQdsRj",
        "outputId": "c9134f7a-2620-4244-9e62-3a4c9f898933"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12\n",
            "314\n"
          ]
        }
      ],
      "source": [
        "print(distance(dataset[0]['src_lang'],dataset[1]['src_lang'],weights=(1,1,2)))\n",
        "\n",
        "#find levenshtein distance between all words\n",
        "lev_distance_list = []\n",
        "word2word_lev = []\n",
        "dataset_len = 0\n",
        "for row in dataset:\n",
        "  word2word_lev = []\n",
        "  for comp_row in dataset:\n",
        "    word1 = row['src_lang']\n",
        "    word2 = comp_row['src_lang']\n",
        "    lev_distance = distance(word1,word2,weights=(1,1,2))\n",
        "    word2word_lev.append(lev_distance)\n",
        "\n",
        "  lev_distance_list.append(word2word_lev)\n",
        "  dataset_len = dataset_len + 1\n",
        "\n",
        "print(dataset_len)\n",
        "#with open(\"/content/drive/MyDrive/Colab Notebooks/NLLB_files/lev_distance_list.json\", 'w') as file:\n",
        "#  json.dump(lev_distance_list, file)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5ZpaL7Hsssi",
        "outputId": "4ba2f2f0-2970-40d1-c219-f33a43071d16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[9, 9, 0, 6, 6, 6, 6, 11, 9, 9, 7, 7, 13, 9, 10, 6, 11, 8, 9, 7, 9, 7, 9, 8, 9, 17, 9, 10, 10, 8, 10, 9, 9, 7, 11, 10, 11, 9, 8, 9, 12, 8, 8, 12, 11, 12, 10, 8, 10, 10, 4, 5, 11, 10, 9, 11, 7, 9, 13, 7, 8, 7, 10, 6, 7, 8, 7, 12, 13, 9, 12, 9, 7, 8, 10, 9, 9, 7, 9, 8, 13, 8, 8, 7, 8, 12, 10, 11, 7, 6, 8, 9, 8, 9, 10, 10, 10, 12, 10, 7, 9, 6, 15, 9, 12, 16, 9, 7, 9, 10, 11, 9, 5, 7, 22, 6, 36, 11, 12, 12, 11, 12, 11, 10, 12, 17, 19, 13, 9, 15, 13, 10, 10, 8, 12, 9, 9, 11, 12, 14, 11, 13, 13, 12, 13, 17, 21, 16, 13, 11, 14, 12, 19, 24, 19, 17, 15, 13, 9, 17, 11, 12, 6, 9, 13, 8, 10, 8, 12, 12, 14, 8, 22, 14, 12, 15, 11, 12, 10, 14, 17, 18, 19, 11, 11, 26, 23, 14, 16, 18, 9, 9, 7, 13, 11, 10, 15, 20, 10, 14, 8, 8, 9, 9, 11, 30, 8, 27, 22, 15, 12, 18, 15, 16, 12, 15, 13, 21, 7, 22, 9, 10, 20, 11, 13, 12, 13, 12, 30, 14, 8, 15, 10, 20, 17, 9, 7, 9, 14, 15, 34, 20, 17, 12, 22, 14, 19, 14, 19, 21, 11, 17, 20, 13, 11, 16, 9, 24, 10, 12, 8, 12, 9, 11, 15, 15, 14, 26, 9, 9, 8, 15, 13, 13, 15, 15, 13, 9, 9, 9, 16, 8, 12, 20, 9, 14, 14, 11, 12, 10, 10, 10, 12, 16, 20, 10, 21, 14, 9, 6, 6, 13, 13, 11, 14, 24, 10, 10, 22, 14, 15, 13, 10, 9]\n"
          ]
        }
      ],
      "source": [
        "print(lev_distance_list[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqz4Ggq5_T9k"
      },
      "source": [
        "\n",
        "1. import googles pretrained word2vec model: about 3 gigs\n",
        "2. sum vectors in each phrase that is the english translation\n",
        "3. find cosine distance between each phrase\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLMgdYerJRcp",
        "outputId": "bebbb146-e77e-440f-8f74-71671aa3d8c6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-06f988ac25a6>:3: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
            "  x=word2vec.word_vec(\"test\")\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "word2vec = KeyedVectors.load_word2vec_format(\"/content/drive/MyDrive/Colab Notebooks/NLLB_files/GoogleNews-vectors-negative300.bin\",binary=True)\n",
        "x=word2vec.word_vec(\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hr4flprQjiNd",
        "outputId": "10cd71d2-8465-4573-ad5e-b2bcc7dcf2e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cats\n"
          ]
        }
      ],
      "source": [
        "# try: print(\"{:.4f}\".format(word2vec.distance(\"media\",\"circus;\")))\n",
        "# except: print(\"word not present\")\n",
        "\n",
        "print((word2vec.similar_by_word(\"cat\")[0])[0])\n",
        "#print(isinstance(word2vec.similar_by_word(\"cat\")[0],KeyedVectors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXtfX-0XqZAi"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "#import cupy as cp\n",
        "split_list = []\n",
        "def vector_sum(translation):\n",
        "    translation_split = re.findall(r'\\b\\w+\\b', translation)\n",
        "    split_list.append(translation_split)\n",
        "    vec_sum = None  # Initialize as None\n",
        "\n",
        "    for word in translation_split:\n",
        "        if word in word2vec:\n",
        "            word_vector = np.asarray(word2vec.get_vector(word))\n",
        "            synonym_list = [ t[0] for t in word2vec.similar_by_word(word)[:3] ]\n",
        "            synonym_vectors = tuple(np.asarray(word2vec.get_vector(s)) for s in synonym_list )\n",
        "            if vec_sum is None:\n",
        "                vec_sum = word_vector  # Initialize vec_sum on first valid word_vector\n",
        "                vec_sum = np.vstack(synonym_vectors + vec_sum)\n",
        "            else:\n",
        "                vec_sum = np.vstack(synonym_vectors + vec_sum)\n",
        "\n",
        "    # Compute average if vec_sum has valid data\n",
        "    if vec_sum is not None:\n",
        "        average_vector = np.mean(vec_sum, axis=0)\n",
        "        return average_vector\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGxlW-tCJFhL",
        "outputId": "0d6cdc1d-bc4a-45d0-be54-8cdf098444d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "print(split_list[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1UkFKWh1mLY1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Preprocess translations and compute average vectors\n",
        "translations = [row[\"tgt_lang\"] for row in dataset]\n",
        "avg_vectors = [vector_sum(translation) for translation in translations]\n",
        "# for i in range(len(avg_vectors)):\n",
        "#   avg_vectors[i] = np.asnumpy(avg_vectors[i])\n",
        "avg_vectorsc = np.empty((len(avg_vectors),len(avg_vectors[0])))\n",
        "for z in range(len(avg_vectors)):\n",
        "  if avg_vectors[z] is None:\n",
        "    avg_vectorsc[z] = -1\n",
        "  else:\n",
        "    avg_vectorsc[z] = avg_vectors[z]\n",
        "\n",
        "cos_list = np.empty((len(avg_vectorsc),len(avg_vectorsc)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-XsMPqChwon-"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Calculate cosine similarities without changing the data structure\n",
        "for i in range(len(dataset)):\n",
        "    #rowList = []\n",
        "    for j in range(len(dataset)):\n",
        "        #avg_t1 = avg_vectorsc[i]\n",
        "        #avg_t2 = avg_vectorsc[j]\n",
        "\n",
        "        if np.any(avg_vectorsc[i] == -1) or np.any(avg_vectorsc[j] == -1):\n",
        "            #rowList.append(-1)\n",
        "            cos_list[i][j] = -1\n",
        "        else:\n",
        "            cos_similarity = np.dot(avg_vectorsc[j], avg_vectorsc[i]) / (np.linalg.norm(avg_vectorsc[j]) * np.linalg.norm(avg_vectorsc[i]))\n",
        "            cos_list[i][j] = cos_similarity\n",
        "    #cos_list.append(rowList)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICw6oBJDVbXB",
        "outputId": "d3a3b997-475c-4df0-efd9-990f82367520"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.02845232270906805\n"
          ]
        }
      ],
      "source": [
        "print(cos_list[0][55])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "WnT1z47ZVNo8",
        "outputId": "37051dfd-cddd-44dc-c610-267de531f40d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'array_type = ctypes.c_float*3144\\ncos_list = (array_type*3144)\\ni = 0\\nj = 0\\nfor row in dataset:\\n  for comp_row in dataset:\\n    #split translation and add vectors together\\n    translation1 = row[\"tgt_lang\"]\\n    translation2 = comp_row[\"tgt_lang\"]\\n\\n    avg_t1 = vector_sum(translation1)\\n    avg_t2 = vector_sum(translation2)\\n\\n    if (avg_t1 is None) or (avg_t2 is None):\\n      #save -1 for cos similarity in matrix\\n      cos_list[i][j] = -1\\n    else:\\n      #save cosign similiarity of avg_t1 and t2 in matrix\\n      cos_similarity = np.dot(avg_t2, avg_t1) / (np.linalg.norm(avg_t2) * np.linalg.norm(avg_t1))\\n      cos_list[i][j]=(round(cos_similarity, 3))\\n    j = j+1\\n  i = i+1 '"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"array_type = ctypes.c_float*3144\n",
        "cos_list = (array_type*3144)\n",
        "i = 0\n",
        "j = 0\n",
        "for row in dataset:\n",
        "  for comp_row in dataset:\n",
        "    #split translation and add vectors together\n",
        "    translation1 = row[\"tgt_lang\"]\n",
        "    translation2 = comp_row[\"tgt_lang\"]\n",
        "\n",
        "    avg_t1 = vector_sum(translation1)\n",
        "    avg_t2 = vector_sum(translation2)\n",
        "\n",
        "    if (avg_t1 is None) or (avg_t2 is None):\n",
        "      #save -1 for cos similarity in matrix\n",
        "      cos_list[i][j] = -1\n",
        "    else:\n",
        "      #save cosign similiarity of avg_t1 and t2 in matrix\n",
        "      cos_similarity = np.dot(avg_t2, avg_t1) / (np.linalg.norm(avg_t2) * np.linalg.norm(avg_t1))\n",
        "      cos_list[i][j]=(round(cos_similarity, 3))\n",
        "    j = j+1\n",
        "  i = i+1 \"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a0rLZdJxPZgQ"
      },
      "outputs": [],
      "source": [
        "# string_cos_list = [[str(item) for item in sublist] for sublist in cos_list]\n",
        "\n",
        "# with open(\"/content/drive/MyDrive/Colab Notebooks/NLLB_files/cos_list.json\", 'w') as file:\n",
        "#   json.dump(string_cos_list, file)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc9Ah0PSFo9d"
      },
      "source": [
        "find if there is a correlation between the LD's of two hittite words and the cosine similarity of their translations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_ZtHS2goHdLx"
      },
      "outputs": [],
      "source": [
        "def flatten_2d_list(lst):\n",
        "    flattened = []\n",
        "    for sublist in lst:\n",
        "        for item in sublist:\n",
        "            flattened.append(item)\n",
        "    return flattened"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ie9FpA_GFoHW"
      },
      "outputs": [],
      "source": [
        "list1_flat = flatten_2d_list(lev_distance_list)\n",
        "list2_flat = np.ndarray.flatten(cos_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sgOrtaHhTSj",
        "outputId": "0d819fda-35e5-4338-8ee9-d916f0a34bfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 1.          0.01731463  0.10101594  0.19417084  0.16399362  0.04719344\n",
            "  0.10734963  0.06959069  0.05550331  0.22216323  0.12887573  0.21562877\n",
            "  0.15159824  0.1311479   0.17157325  0.32367978  0.36243549 -1.\n",
            "  0.28568952  0.14399579  0.1117714   0.12984882  0.19722738  0.10101594\n",
            "  0.03606979  0.10040741 -0.00225847  0.06236852  0.48722242 -0.01574382\n",
            "  0.27364922  0.26132209  0.18029661  0.10101594  0.16879469  0.04403303\n",
            "  0.16474359  0.03604436  0.05737092  0.05983309  0.13862018  0.13573963\n",
            "  0.12458569  0.16654572  0.16654572  0.10101594  0.26627378  0.1416164\n",
            "  0.05434749  0.12272297  0.16522835  0.06339641  0.10740456  0.03549941\n",
            "  0.06592603]\n"
          ]
        }
      ],
      "source": [
        "print(list2_flat[:55])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AXp5UQqG0zCd"
      },
      "outputs": [],
      "source": [
        "list3_flat = []\n",
        "for i in range(len(list2_flat)):\n",
        "  list3_flat.append(str(list2_flat[i]))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dYJFfjxX1Lz_"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/NLLB_files/flat_cos_list_2.json', 'w') as file:\n",
        "    json.dump(list2_flat.tolist(), file)\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/NLLB_files/flat_lev_distance_list.json', 'w') as file:\n",
        "    json.dump(list1_flat, file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "q77pEAqYwcpw",
        "outputId": "e78f7380-e3da-4a4c-e9dd-83dc53890026"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.02845232270906805\n"
          ]
        }
      ],
      "source": [
        "print(list2_flat[55])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ippXzjqKGRvT",
        "outputId": "1c413742-3053-45fc-9dd5-d291fac1303e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pearson correlation coefficient: 0.018444874575216992\n",
            "P-value: 6.9507642602152014e-09\n"
          ]
        }
      ],
      "source": [
        "# !pip install scipy\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "correlation_coefficient, p_value = pearsonr(list1_flat, list2_flat)\n",
        "\n",
        "print(f\"Pearson correlation coefficient: {correlation_coefficient}\")\n",
        "print(f\"P-value: {p_value}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}